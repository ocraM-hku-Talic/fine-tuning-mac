pip install mlx-lm
python grab_data.py

mlx_lm.generate --prompt "[chat content]" --model [huggingface model]

testing model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B

commands:
python material_generation.py --slides_dir "course_materials" --use_ollama --ollama_model "deepseek-r1:latest" --num_samples 500
python clean_qa_data.py
mlx_lm.lora --model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" --train --data "./cleaned-dataset" --iters 100 --num-layer 32

# Just use the MLX fine-tuned model
python compare_models.py --prompt "What is tort law?" --mlx-only
# Just use the Ollama model
python compare_models.py --prompt "What is tort law?" --ollama-model "deepseek-r1:latest" --ollama-only
#both models
python compare_models.py --prompt "What is tort law?" --ollama-model "deepseek-r1:latest"
# Control temperature and max tokens
python compare_models.py --prompt "What is tort law?" --temp 0.2 --max-tokens 1000

=======================================================
after prep model:
--fine-tune-type (default LoRA)
--num-layers (default 16)
--batch-size 
--grad-checkpoint

after tuning:
From ./model
ADAPTER ./adapters